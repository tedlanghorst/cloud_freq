{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b11ac8dc-bed1-4a0f-ba9d-f5fe1c5e24a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2765189/2723989440.py:30: FutureWarning: The geopandas.dataset module is deprecated and will be removed in GeoPandas 1.0. You can get the original 'naturalearth_lowres' data from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/.\n",
      "  world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "%matplotlib widget\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm, Normalize, LogNorm\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "import textwrap\n",
    "\n",
    "import functions_calcs as fc\n",
    "import functions_plotting as fp\n",
    "\n",
    "SAVE_PLOTS = True\n",
    "\n",
    "proj_path = Path('/work/pi_kandread_umass_edu/Cloud_Freq/')\n",
    "figure_path = proj_path / \"figures\" \n",
    "\n",
    "parquet_file = proj_path / 'data' / 'grades_level2.parquet'\n",
    "site_file = proj_path / 'data' / 'MERIT_sites' / 'merit_stratified_discharge_level2_sample.shp'\n",
    "\n",
    "hydroBasins_files = glob('/work/pi_kandread_umass_edu/Datasets/HydroBASINS/*/*lev02*.shp')\n",
    "\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world = world.to_crs('World_Robinson')\n",
    "\n",
    "daily_data_file = proj_path / 'data' / 'analysis_data_daily.parquet'\n",
    "sites_data_file = proj_path / 'data' / 'analysis_data_sites.parquet'\n",
    "basins_data_file = proj_path / 'data' / 'analysis_data_basins.parquet'\n",
    "resampled_sites_data_file = proj_path / 'data' / 'analysis_data_resampled_sites.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938ed93-8aa2-4794-aafb-ca61f2ede94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(fc)\n",
    "importlib.reload(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3486c5a-8d54-4697-81b2-ab6734d67100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in pre-processed daily data\n",
    "df = pd.read_parquet(parquet_file, engine='pyarrow')\n",
    "\n",
    "# Calculate 30-day rolling temp and use Xiao's ice mode\n",
    "temp30 = (df.groupby('id')['temperature_2m']\n",
    "          .rolling(window=30, min_periods=1)\n",
    "          .mean()\n",
    "          .reset_index(level=0, drop=True))-273.15\n",
    "period = (df.index.get_level_values('time').month.isin([8, 9, 10, 11, 12, 1])).astype(int)\n",
    "log_ice = (-0.32*temp30) + (-0.05*temp30*period) + -0.82\n",
    "df['pIce'] = np.exp(log_ice)\n",
    "\n",
    "df[df.pIce>0.5] = np.nan\n",
    "df[df.Q<=0] = np.nan\n",
    "df.dropna(subset=['Q','cloudMask'], inplace=True)\n",
    "\n",
    "#Remove basins with mean Q < 1\n",
    "ids_to_remove = df.groupby('id')['Q'].mean()[lambda x: x < 1].index\n",
    "df= df[~df.index.get_level_values('id').isin(ids_to_remove)]\n",
    "\n",
    "# read in site data\n",
    "sites = gpd.read_file(site_file).set_index('id')\n",
    "# median q and cloud values per site\n",
    "sites = sites.join(df.groupby('id').median()).to_crs('World_Robinson')\n",
    "\n",
    "# Define cloud classes\n",
    "bins = [0, 0.25, 0.75, 1]\n",
    "labels = ['No Cloud', 'Mixed', 'Cloud']\n",
    "# Use pd.cut to map values to bins\n",
    "df['cloud_class'] = pd.cut(df['cloudMask'], bins=bins, labels=labels, right=False)\n",
    "df['cloud_binary'] = df['cloud_class'] != 'No Cloud'\n",
    "\n",
    "# Rank the 'Q' within each site\n",
    "df['Q_norm'] = fc.calc_quantiles(df)\n",
    "\n",
    "# Create bins from normalized flow data\n",
    "num_bins = 10\n",
    "bin_edges = np.linspace(0, 1, num_bins+1)\n",
    "df['quantile_bin'] = np.digitize(df['Q_norm'], bins=bin_edges, right=True)\n",
    "\n",
    "df['month'] = df.index.get_level_values('time').month\n",
    "\n",
    "# Calculate normalized differences\n",
    "quants = [0, 1, 5, 10, 50, 90, 95, 99, 100]\n",
    "for q in quants:\n",
    "    sites = fc.calc_masked_norm_diff(sites, df['Q'], df['cloud_binary'], q/100, f'normDiff_q{q:02.0f}')\n",
    "\n",
    "sites = sites.join(fc.calc_spectral_props(df))\n",
    "\n",
    "#Read in hydrobasins\n",
    "gdf_list = []\n",
    "for file in hydroBasins_files:\n",
    "    gdf_list.append(gpd.read_file(file))\n",
    "hydroBasins = pd.concat(gdf_list, ignore_index=True).set_index('PFAF_ID')\n",
    "\n",
    "#clipping removes antimeridian crossovers in reprojection\n",
    "hydroBasins = gpd.clip(hydroBasins, [-180, -90, 180, 90]).to_crs('World_Robinson')\n",
    "\n",
    "#Merge level2 average values to hydrobasins\n",
    "sites['l1'] = sites.index//1E7\n",
    "sites['l2'] = sites.index//1E6\n",
    "level2_values = sites.groupby('l2').median(numeric_only=True)\n",
    "hydroBasins = hydroBasins.merge(level2_values, left_index=True, right_index=True)\n",
    "\n",
    "# # Calculate seasonal amplitude and phase\n",
    "# monthly = df.groupby(['id','month']).agg({\n",
    "#     'Q_norm': ['count','mean'],   \n",
    "#     'cloudMask': 'mean'\n",
    "# })\n",
    "# monthly.columns = ['count', 'Q_norm', 'cloudMask']\n",
    "# sites['amp'] = np.nan\n",
    "# sites['offset'] = np.nan\n",
    "# for idx, g in monthly.groupby('id'):\n",
    "#     mask = g['count'] > (g['count'].max() * 0.1)\n",
    "#     tmp = g[mask]   \n",
    "#     if len(tmp) != 0:\n",
    "#         phase_offset, amp = fc.calc_phase_amp(tmp['cloudMask'],tmp['Q_norm'])    \n",
    "#     if ~np.isnan(amp):    \n",
    "#         sites.loc[idx,'amp'] = amp\n",
    "#         sites.loc[idx,'offset'] = phase_offset\n",
    "\n",
    "#Read in the Krabbenhoft 2020 network bias data\n",
    "tmp = pd.read_csv(proj_path / 'data' / \"Krabbenhoft\" / \"Attribute_table.csv\",low_memory=False)\n",
    "tmp = tmp.set_index('COMID')['stationid']\n",
    "sites = sites.join(tmp)\n",
    "\n",
    "quants = [0, 0.01, 0.05, 0.10, 0.50, 0.90, 0.95, 0.99, 1.00]\n",
    "windows = ['1D','3D','7D','14D','1ME','2ME','3ME','6ME','1YE']\n",
    "resampled_sites = fc.resampled_norm_diff(sites,df,windows,quants)\n",
    "\n",
    "df.to_parquet(daily_data_file, engine='pyarrow')\n",
    "sites.to_parquet(sites_data_file, engine='pyarrow')\n",
    "hydroBasins.to_parquet(basins_data_file, engine='pyarrow')\n",
    "resampled_sites.to_pickle(resampled_sites_data_file)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd86513-7524-4a32-916c-52e8517c06cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(daily_data_file, engine='pyarrow')\n",
    "sites = gpd.read_parquet(sites_data_file)\n",
    "hydroBasins = gpd.read_parquet(basins_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3618b732-042f-4473-819b-6d62fa4e10db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>cloudMask</th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>Q</th>\n",
       "      <th>pIce</th>\n",
       "      <th>cloud_class</th>\n",
       "      <th>cloud_binary</th>\n",
       "      <th>Q_rank</th>\n",
       "      <th>Q_norm</th>\n",
       "      <th>quantile_bin</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-02-26</th>\n",
       "      <th>11004678</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>305.050198</td>\n",
       "      <td>0.019567</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>No Cloud</td>\n",
       "      <td>False</td>\n",
       "      <td>2094.0</td>\n",
       "      <td>0.248841</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-27</th>\n",
       "      <th>11004678</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>303.389604</td>\n",
       "      <td>0.019322</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>No Cloud</td>\n",
       "      <td>False</td>\n",
       "      <td>2074.0</td>\n",
       "      <td>0.246463</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-28</th>\n",
       "      <th>11004678</th>\n",
       "      <td>0.003130</td>\n",
       "      <td>301.492652</td>\n",
       "      <td>0.020493</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>No Cloud</td>\n",
       "      <td>False</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.258352</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-03-01</th>\n",
       "      <th>11004678</th>\n",
       "      <td>0.040691</td>\n",
       "      <td>301.278195</td>\n",
       "      <td>0.021155</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>No Cloud</td>\n",
       "      <td>False</td>\n",
       "      <td>2242.0</td>\n",
       "      <td>0.266437</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-03-02</th>\n",
       "      <th>11004678</th>\n",
       "      <td>0.679383</td>\n",
       "      <td>300.944331</td>\n",
       "      <td>0.020203</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>True</td>\n",
       "      <td>2150.0</td>\n",
       "      <td>0.255499</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-27</th>\n",
       "      <th>91006323</th>\n",
       "      <td>0.069345</td>\n",
       "      <td>272.328320</td>\n",
       "      <td>15.021329</td>\n",
       "      <td>0.209224</td>\n",
       "      <td>No Cloud</td>\n",
       "      <td>False</td>\n",
       "      <td>208.0</td>\n",
       "      <td>0.192917</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-28</th>\n",
       "      <th>91006323</th>\n",
       "      <td>0.005727</td>\n",
       "      <td>271.535291</td>\n",
       "      <td>15.046756</td>\n",
       "      <td>0.224514</td>\n",
       "      <td>No Cloud</td>\n",
       "      <td>False</td>\n",
       "      <td>217.0</td>\n",
       "      <td>0.201305</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-29</th>\n",
       "      <th>91006323</th>\n",
       "      <td>0.103834</td>\n",
       "      <td>269.996463</td>\n",
       "      <td>15.059526</td>\n",
       "      <td>0.240861</td>\n",
       "      <td>No Cloud</td>\n",
       "      <td>False</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0.205033</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-30</th>\n",
       "      <th>91006323</th>\n",
       "      <td>0.007394</td>\n",
       "      <td>267.475321</td>\n",
       "      <td>15.037016</td>\n",
       "      <td>0.266940</td>\n",
       "      <td>No Cloud</td>\n",
       "      <td>False</td>\n",
       "      <td>213.0</td>\n",
       "      <td>0.197577</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-31</th>\n",
       "      <th>91006323</th>\n",
       "      <td>0.106948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.986571</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>No Cloud</td>\n",
       "      <td>False</td>\n",
       "      <td>199.0</td>\n",
       "      <td>0.184529</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141136058 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     cloudMask  temperature_2m          Q      pIce  \\\n",
       "time       id                                                         \n",
       "2000-02-26 11004678   0.000000      305.050198   0.019567  0.000016   \n",
       "2000-02-27 11004678   0.000000      303.389604   0.019322  0.000021   \n",
       "2000-02-28 11004678   0.003130      301.492652   0.020493  0.000028   \n",
       "2000-03-01 11004678   0.040691      301.278195   0.021155  0.000033   \n",
       "2000-03-02 11004678   0.679383      300.944331   0.020203  0.000038   \n",
       "...                        ...             ...        ...       ...   \n",
       "2023-08-27 91006323   0.069345      272.328320  15.021329  0.209224   \n",
       "2023-08-28 91006323   0.005727      271.535291  15.046756  0.224514   \n",
       "2023-08-29 91006323   0.103834      269.996463  15.059526  0.240861   \n",
       "2023-08-30 91006323   0.007394      267.475321  15.037016  0.266940   \n",
       "2023-08-31 91006323   0.106948             NaN  14.986571  0.269231   \n",
       "\n",
       "                    cloud_class  cloud_binary  Q_rank    Q_norm  quantile_bin  \\\n",
       "time       id                                                                   \n",
       "2000-02-26 11004678    No Cloud         False  2094.0  0.248841             3   \n",
       "2000-02-27 11004678    No Cloud         False  2074.0  0.246463             3   \n",
       "2000-02-28 11004678    No Cloud         False  2174.0  0.258352             3   \n",
       "2000-03-01 11004678    No Cloud         False  2242.0  0.266437             3   \n",
       "2000-03-02 11004678       Mixed          True  2150.0  0.255499             3   \n",
       "...                         ...           ...     ...       ...           ...   \n",
       "2023-08-27 91006323    No Cloud         False   208.0  0.192917             2   \n",
       "2023-08-28 91006323    No Cloud         False   217.0  0.201305             3   \n",
       "2023-08-29 91006323    No Cloud         False   221.0  0.205033             3   \n",
       "2023-08-30 91006323    No Cloud         False   213.0  0.197577             2   \n",
       "2023-08-31 91006323    No Cloud         False   199.0  0.184529             2   \n",
       "\n",
       "                     month  \n",
       "time       id               \n",
       "2000-02-26 11004678      2  \n",
       "2000-02-27 11004678      2  \n",
       "2000-02-28 11004678      2  \n",
       "2000-03-01 11004678      3  \n",
       "2000-03-02 11004678      3  \n",
       "...                    ...  \n",
       "2023-08-27 91006323      8  \n",
       "2023-08-28 91006323      8  \n",
       "2023-08-29 91006323      8  \n",
       "2023-08-30 91006323      8  \n",
       "2023-08-31 91006323      8  \n",
       "\n",
       "[141136058 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284fd6b4-5402-42d0-90fe-b535548c2e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "fp.cloud_bar_plot(df, ax)\n",
    "plt.tight_layout()\n",
    "\n",
    "if SAVE_PLOTS:\n",
    "    fig.savefig(figure_path / \"cloudiness_all.png\", format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c6674-626b-458d-97b0-657cfd5b278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "quant = 50\n",
    "norm = TwoSlopeNorm(vmin=-0.4, vcenter=0, vmax=0.4)\n",
    "cmap = 'RdBu'\n",
    "\n",
    "fp.pfaf_level2_plot(hydroBasins,f'normDiff_q{quant:02.0f}',cmap,norm,ax)\n",
    "\n",
    "plt.title(f\"Normalized Difference at {quant:02.0f}% Quantile\")\n",
    "\n",
    "fig.colorbar(mp.cm.ScalarMappable(norm=norm, cmap=cmap), \n",
    "             ax=ax, \n",
    "             fraction=0.05, \n",
    "             shrink=0.7, \n",
    "             pad=0.05,\n",
    "             label='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363455ef-90cd-4f81-a745-38c1ea2ee5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "norm = TwoSlopeNorm(vmin=-0.5, vcenter=0, vmax=0.5)\n",
    "cmap = 'RdBu'\n",
    "    \n",
    "fig, axes = plt.subplots(3,3,figsize=(10,5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, quant in tqdm(zip(axes,quants),total=len(axes)):\n",
    "    fp.pfaf_level2_plot(hydroBasins,f'normDiff_q{quant:02.0f}',cmap, norm, ax)\n",
    "    ax.set_title(f\"{quant:2.0f}%\",y=0.9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.subplots_adjust(left=0.05, right=0.85, top=0.95, bottom=0.05, wspace=0, hspace=0)\n",
    "cax = fig.add_axes([0.87, 0.2, 0.02, 0.6])\n",
    "cbar = fig.colorbar(mp.cm.ScalarMappable(norm=norm, cmap=cmap), cax=cax)\n",
    "cbar.set_ticks([-0.4, -0.2, 0, 0.2, 0.4])\n",
    "cbar.set_label(\"Normalized difference\")\n",
    "\n",
    "if SAVE_PLOTS:\n",
    "    fig.savefig(figure_path / \"3x3_maps.png\", format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c37dd-327a-4be8-88b4-9aee944847fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weird_id = 2260400 #IRRAWADDY RIVER\n",
    "weird_site = sites.sample(1)\n",
    "weird_id = weird_site.index[0]\n",
    "\n",
    "weird_df = df[df.index.get_level_values('id')==weird_id]\n",
    "\n",
    "plt.close('all')\n",
    "fig, axes = plt.subplot_mosaic([['top','top'],['left', 'right']],\n",
    "                              constrained_layout=True)\n",
    "\n",
    "\n",
    "axes['top'].plot(weird_df.index.get_level_values('time'),\n",
    "         weird_df['Q'])\n",
    "axes['top'].scatter(weird_df.index.get_level_values('time')[~weird_df.cloud_binary],\n",
    "            weird_df['Q'][~weird_df.cloud_binary],color='orange')\n",
    "\n",
    "fp.cloud_bar_plot(weird_df, axes['left'])\n",
    "\n",
    "world.plot(ax=axes['right'], color='lightgray')\n",
    "weird_site.to_crs('World_Robinson').plot(ax=axes['right'])\n",
    "# axes['right'].square()\n",
    "axes['right'].set_xlim([-1.4E7, 1.6E7])\n",
    "axes['right'].set_ylim([-6E6, 8E6])\n",
    "axes['right'].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a7ed1-cf41-4a7a-997b-046fb36bb99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "norm = TwoSlopeNorm(vmin=-0.5, vcenter=0, vmax=0.5)\n",
    "cmap = 'RdBu'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "ax.scatter(sites['amp'],\n",
    "            sites['offset']+np.random.rand(len(sites))*0,\n",
    "            c=sites['normDiff_q50'],s=0.1,norm=norm, cmap='RdBu',edgecolor=None)\n",
    "ax.set_ylabel(\"Phase offset\")\n",
    "ax.set_xlabel(\"Mean amplitude\")\n",
    "ax.set_xlim([0.05,0.5])\n",
    "# ax.set_facecolor([0.75]*3)\n",
    "\n",
    "plt.subplots_adjust(left=0.1, right=0.8)\n",
    "cax = fig.add_axes([0.85, 0.15, 0.03, 0.7])\n",
    "cbar = fig.colorbar(mp.cm.ScalarMappable(norm=norm, cmap=cmap), cax=cax)\n",
    "cbar.set_ticks([-0.4, -0.2, 0, 0.2, 0.4])\n",
    "cbar.set_label(\"Norm. diff. of median discharge\")\n",
    "\n",
    "if SAVE_PLOTS:\n",
    "    fig.savefig(figure_path / \"phase_amplitude_scatter.png\", format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec5b9b5-90ac-434b-a9e0-0ab67bac490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "col_plot = 'uparea'\n",
    "river_sizes = [0,1E2,1E3,1E4,1E5,1E6,np.inf] #basin size\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "\n",
    "# Create a color cycle with a continuous colormap\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(river_sizes)-1))\n",
    "ax.set_prop_cycle('color', colors)\n",
    "\n",
    "# Plot each site ID's time series\n",
    "for lower_limit, upper_limit in zip(river_sizes, river_sizes[1:]):\n",
    "    # sites_size_bin = sites[(sites['lta_discharge']>=lower_limit) & (sites['lta_discharge']<upper_limit)]\n",
    "    sites_size_bin = sites[(sites[col_plot]>=lower_limit) & (sites[col_plot]<upper_limit)]\n",
    "    df_size_bin = df[df.index.get_level_values('id').isin(sites_size_bin.index)]\n",
    "\n",
    "    df_grouped = df_size_bin.groupby('quantile_bin')['cloud_class'].value_counts(normalize=True).unstack('cloud_class')\n",
    "    df_grouped = df_grouped[(df_grouped.index > 0) & (df_grouped.index<11)]\n",
    "\n",
    "\n",
    "    ax.plot(df_grouped.index,df_grouped['No Cloud'],label=f\"{lower_limit/1E3:.0f}-{upper_limit/1E3:.0f}, n={len(sites_size_bin)}\")\n",
    "\n",
    "ax.set_xticks(np.linspace(1,10,10))\n",
    "new_labels = [f'{label * 0.1:.1f}' for label in df_grouped.index]\n",
    "ax.set_xticklabels(new_labels, rotation=0)\n",
    "\n",
    "# Create a twin Axes to duplicate y labels\n",
    "ax2 = ax.twinx()\n",
    "ax2.set_yticks(ax.get_yticks())\n",
    "ax2.set_ylim(ax.get_ylim())\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "ax.legend(title='Area (1000 km\\u00B2)')\n",
    "ax.set_xlabel('Discharge Quantile')\n",
    "ax.set_ylabel('Fraction of cloud-free images')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if SAVE_PLOTS:\n",
    "    fig.savefig(figure_path / \"cloudiness_by_area.png\", format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810ef70b-200a-4e66-b128-64069862e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "col_plot = 'Q'\n",
    "river_sizes = [1E0, 1E1, 1E2, 1E3, 1E4, 1E6]\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "\n",
    "# Create a color cycle with a continuous colormap\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(river_sizes)-1))\n",
    "ax.set_prop_cycle('color', colors)\n",
    "\n",
    "# Plot each site ID's time series\n",
    "for lower_limit, upper_limit in zip(river_sizes, river_sizes[1:]):\n",
    "    # sites_size_bin = sites[(sites['lta_discharge']>=lower_limit) & (sites['lta_discharge']<upper_limit)]\n",
    "    sites_size_bin = sites[(sites[col_plot]>=lower_limit) & (sites[col_plot]<upper_limit)]\n",
    "    df_size_bin = df[df.index.get_level_values('id').isin(sites_size_bin.index)]\n",
    "\n",
    "    df_grouped = df_size_bin.groupby('quantile_bin')['cloud_class'].value_counts(normalize=True).unstack('cloud_class')\n",
    "    df_grouped = df_grouped[(df_grouped.index > 0) & (df_grouped.index < 11)]\n",
    "\n",
    "    low_string = f'$10^{np.log10(lower_limit):1.0f}$'\n",
    "    high_string = f'$10^{np.log10(upper_limit):1.0f}$'\n",
    "    \n",
    "    ax.plot(df_grouped.index,df_grouped['No Cloud'],label=f\"{low_string}-{high_string}, n={len(sites_size_bin)}\")\n",
    "    print(df_grouped['No Cloud'])\n",
    "\n",
    "ax.set_xticks(np.linspace(2,10,5))\n",
    "new_labels = [f'{label * 0.1:.1f}' for label in ax.get_xticks()]\n",
    "ax.set_xticklabels(new_labels, rotation=0)\n",
    "\n",
    "ax.set_ylim([0.23,0.6])\n",
    "ax.set_yticks(np.linspace(0.25,0.55,4))\n",
    "\n",
    "# # Create a twin Axes to duplicate y labels\n",
    "# ax2 = ax.twinx()\n",
    "# ax2.set_yticks(ax.get_yticks())\n",
    "# ax2.set_ylim(ax.get_ylim())\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(left=0.1, right=0.5, bottom=0.2)\n",
    "ax.legend(title='Discharge [$m^3/s$]', loc='upper left', bbox_to_anchor=(1.2, 1))\n",
    "ax.set_xlabel('Discharge Quantile')\n",
    "ax.set_ylabel('Fraction of cloud-free images')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if SAVE_PLOTS:\n",
    "    fig.savefig(figure_path / \"cloudiness_by_discharge.png\", format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4481ffd-3d17-4f18-884a-43bb35579ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "tmp.correlation.hist(range=(-.6,.6),bins=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b71f6-a18b-40a0-8e22-e3a2f1277cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "col_plot = 'temperature_2m'\n",
    "river_sizes = [0,10,15,20,25,30]\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "\n",
    "# Create a color cycle with a continuous colormap\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(river_sizes)-1))\n",
    "ax.set_prop_cycle('color', colors)\n",
    "\n",
    "# Plot each site ID's time series\n",
    "for lower_limit, upper_limit in zip(river_sizes, river_sizes[1:]):\n",
    "    sites_size_bin = sites[(sites[col_plot]>=(lower_limit+273.15)) & (sites[col_plot]<(upper_limit+273.15))]\n",
    "    df_size_bin = df[df.index.get_level_values('id').isin(sites_size_bin.index)]\n",
    "\n",
    "    df_grouped = df_size_bin.groupby('quantile_bin')['cloud_class'].value_counts(normalize=True).unstack('cloud_class')\n",
    "    df_grouped = df_grouped[(df_grouped.index > 0) & (df_grouped.index < 11)]\n",
    "\n",
    "    low_string = f'{lower_limit}'\n",
    "    high_string = f'{upper_limit}'\n",
    "    \n",
    "    ax.plot(df_grouped.index,df_grouped['No Cloud'],label=f\"{low_string} - {high_string}, n={len(sites_size_bin)}\")\n",
    "\n",
    "ax.set_xticks(np.linspace(2,10,5))\n",
    "new_labels = [f'{label * 0.1:.1f}' for label in ax.get_xticks()]\n",
    "ax.set_xticklabels(new_labels, rotation=0)\n",
    "\n",
    "ax.set_ylim([0.23,0.6])\n",
    "ax.set_yticks(np.linspace(0.25,0.55,4))\n",
    "\n",
    "# # Create a twin Axes to duplicate y labels\n",
    "# ax2 = ax.twinx()\n",
    "# ax2.set_yticks(ax.get_yticks())\n",
    "# ax2.set_ylim(ax.get_ylim())\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(left=0.1, right=0.5, bottom=0.2)\n",
    "ax.legend(title='Temperature [C]', loc='upper left', bbox_to_anchor=(1.2, 1))\n",
    "ax.set_xlabel('Discharge Quantile')\n",
    "ax.set_ylabel('Fraction of cloud-free images')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if SAVE_PLOTS:\n",
    "    fig.savefig(figure_path / \"cloudiness_by_temperature.png\", format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2bed0e-e897-4244-87c5-a8de34334e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "col_plot = 'slope'\n",
    "river_sizes = [1E-6, 1E-4, 1E-3, 1E-2, 1E0]\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "\n",
    "# Create a color cycle with a continuous colormap\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(river_sizes)-1))\n",
    "ax.set_prop_cycle('color', colors)\n",
    "\n",
    "# Plot each site ID's time series\n",
    "for lower_limit, upper_limit in zip(river_sizes, river_sizes[1:]):\n",
    "    # sites_size_bin = sites[(sites['lta_discharge']>=lower_limit) & (sites['lta_discharge']<upper_limit)]\n",
    "    sites_size_bin = sites[(sites[col_plot]>=lower_limit) & (sites[col_plot]<upper_limit)]\n",
    "    df_size_bin = df[df.index.get_level_values('id').isin(sites_size_bin.index)]\n",
    "\n",
    "    df_grouped = df_size_bin.groupby('quantile_bin')['cloud_class'].value_counts(normalize=True).unstack('cloud_class')\n",
    "    df_grouped = df_grouped[(df_grouped.index > 0) & (df_grouped.index < 11)]\n",
    "\n",
    "    low_string = f'$10^{{ {np.log10(lower_limit):1.0f} }}$'\n",
    "    high_string = f'$10^{{ {np.log10(upper_limit):1.0f} }}$'\n",
    "    \n",
    "    ax.plot(df_grouped.index,df_grouped['No Cloud'],label=f\"{low_string}-{high_string}, n={len(sites_size_bin)}\")\n",
    "\n",
    "ax.set_xticks(np.linspace(2,10,5))\n",
    "new_labels = [f'{label * 0.1:.1f}' for label in ax.get_xticks()]\n",
    "ax.set_xticklabels(new_labels, rotation=0)\n",
    "\n",
    "ax.set_ylim([0.23,0.6])\n",
    "ax.set_yticks(np.linspace(0.25,0.55,4))\n",
    "\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(left=0.1, right=0.5, bottom=0.2)\n",
    "ax.legend(title='Slope [1/m]', loc='upper left', bbox_to_anchor=(1.2, 1))\n",
    "ax.set_xlabel('Discharge Quantile')\n",
    "ax.set_ylabel('Fraction of cloud-free images')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if SAVE_PLOTS:\n",
    "    fig.savefig(figure_path / \"cloudiness_by_slope.png\", format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af1098-3fa1-4a9f-a2be-b22ca2b94bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = []\n",
    "corr = []\n",
    "for i,g in tqdm(df.groupby('id')):\n",
    "    g.dropna(subset=['cloudMask','Q'], inplace=True)\n",
    "    \n",
    "    idx.append(i)\n",
    "    corr.append(np.corrcoef(g['cloudMask'],g['Q'])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92ff6a-07ce-4257-ae28-373670c3f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({'correlation': corr},index=idx)\n",
    "tmp = sites.join(tmp)\n",
    "tmp['log_slope'] = np.log10(tmp.slope)\n",
    "tmp['log_Q'] = np.log10(tmp.Q)\n",
    "tmp.dropna(subset=['slope','Q','temperature_2m','correlation'],inplace=True)\n",
    "\n",
    "plt.close('all')\n",
    "tmp.plot.scatter('temperature_2m','correlation',c='normDiff_q50',s=0.25,cmap=cmap,norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01dc3d-c8ff-4a71-a545-989043d4d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "quants = [0, 1, 5, 10, 50, 90, 95, 99, 100]\n",
    "fig, axes = plt.subplots(3,3,figsize=(10,10))\n",
    "axes = axes.flatten() # Flatten the axes array for easy iteration\n",
    "\n",
    "sites_plot = sites\n",
    "\n",
    "for (quant, ax) in tqdm(zip(quants, axes), total=len(quants)): \n",
    "    fp.obs_true_1to1(df,quant,ax)\n",
    "    \n",
    "fig.supxlabel('All Discharge [$m^3/s$]', fontsize=14)\n",
    "fig.supylabel('Cloud-Free Discharge [$m^3/s$]', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "if SAVE_PLOTS:\n",
    "    fig.savefig(figure_path / \"3x3_scatterplots.png\", format='png', dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a6070-647d-4771-bd6a-4be2f562a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find length of cloud-covered periods\n",
    "def cloud_covered_periods(data):\n",
    "    periods = []\n",
    "    current_period = 0\n",
    "\n",
    "    for value in data:\n",
    "        if value:  # Cloud covered\n",
    "            current_period += 1\n",
    "        else:  # Not cloud covered\n",
    "            periods.append(current_period)\n",
    "            current_period = 0\n",
    "\n",
    "    return np.quantile(periods,[q/100.0 for q in quants])\n",
    "    # return np.mean(periods)\n",
    "\n",
    "# Apply the function to each 'cloud_binary' series\n",
    "cloud_periods = df.groupby('id')['cloud_binary'].apply(cloud_covered_periods)\n",
    "\n",
    "tmp = pd.DataFrame(cloud_periods.tolist(), index=cloud_periods.index)\n",
    "tmp.columns = [f\"q{q:02.0f}\" for q in quants]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed8b46e-c4a5-492f-bb0d-f1b217453111",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "norm = Normalize(vmin=0, vmax=30)\n",
    "cmap = 'inferno'\n",
    "\n",
    "sites_tmp = sites.join(tmp)\n",
    "sites_tmp.plot('q90',markersize=1,ax=ax,cmap=cmap,norm=norm)\n",
    "\n",
    "plt.subplots_adjust(left=0.05, right=0.85, top=0.95, bottom=0.05, wspace=0, hspace=0)\n",
    "cax = fig.add_axes([0.87, 0.2, 0.02, 0.6])\n",
    "cbar = fig.colorbar(mp.cm.ScalarMappable(norm=norm, cmap=cmap), cax=cax)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739fe837-4039-4c88-a52a-fb238d3fc456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch, coherence, csd\n",
    "plt.close('all')\n",
    "\n",
    "\n",
    "def spectral_analysis_site(df, site_id):\n",
    "    # Filter data for the specific site\n",
    "    site_data = df.xs(site_id, level='id')[['Q_norm','cloudMask']]\n",
    "    \n",
    "    #number of valid days per year. Excludes missing data or ice flagged days.\n",
    "    nperseg = 365.25*len(site_data)/(site_data.index.max()-site_data.index.min()).days\n",
    "\n",
    "    # Preprocessing: Interpolate missing values and ensure uniform sampling\n",
    "    site_data = site_data#.resample('D').interpolate()\n",
    "\n",
    "    # Extract discharge (Q) and cloud cover (cloudMask) columns\n",
    "    discharge = site_data['Q_norm'].values\n",
    "    cloud_cover = site_data['cloudMask'].values\n",
    "    \n",
    "    # discharge = (discharge-np.mean(discharge))/np.std(discharge)\n",
    "\n",
    "    # Spectral analysis\n",
    "    fs = 1  # Sampling frequency (1 sample per day)\n",
    "    f_discharge, Pxx_discharge = welch(discharge, fs=fs, nperseg=nperseg, scaling='density')\n",
    "    f_cloud, Pxx_cloud = welch(cloud_cover, fs=fs, nperseg=nperseg, scaling='density')\n",
    "    f_coherence, Cxy = coherence(discharge, cloud_cover, fs=fs, nperseg=nperseg)\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplot_mosaic([['ts','ts','ts'],\n",
    "                                   ['l','c','r']],\n",
    "                                   figsize = (8,4))\n",
    "\n",
    "    # Discharge and cloud cover time series\n",
    "    ax['ts'].plot(site_data.index, discharge, label='Discharge')\n",
    "    # ax['ts'].plot(site_data.index, cloud_cover, label='Cloud Cover')\n",
    "    # ax['r'].xlabel('Time')\n",
    "    # plt.ylabel('Value')\n",
    "    # plt.title(f'Time Series (Site {site_id})')\n",
    "    # plt.legend()\n",
    "\n",
    "    # Spectral power density\n",
    "    ax['l'].semilogy(f_discharge, Pxx_discharge, label='Discharge')\n",
    "    ax['l'].semilogy(f_cloud, Pxx_cloud, label='Cloud Cover')\n",
    "    # plt.xlabel('Frequency (cycles/day)')\n",
    "    # plt.ylabel('Power Density')\n",
    "    # plt.title('Spectral Power Density')\n",
    "    # plt.legend()\n",
    "\n",
    "    # Coherence\n",
    "    ax['c'].plot(f_coherence, Cxy)\n",
    "    # ax['r'].xlabel('Frequency (cycles/day)')\n",
    "    # ax['r'].set_ylabel('Coherence')\n",
    "    # ax['r'].title('Coherence between Discharge and Cloud Cover')\n",
    "    \n",
    "    # Calculate cross spectral density which contains phase information\n",
    "    f, Pxy = csd(discharge, cloud_cover, fs=fs, nperseg=nperseg)\n",
    "\n",
    "    # Calculate the phase spectrum in degrees\n",
    "    phase_spectrum = np.angle(Pxy, deg=True)\n",
    "\n",
    "    # Plot the phase spectrum\n",
    "    ax['r'].plot(f, phase_spectrum,'.')  # Plotting against period (1/frequency)\n",
    "    # ax['r'].xlabel('Period (days)')\n",
    "    # plt.ylabel('Phase Difference (degrees)')\n",
    "    # plt.title('Phase Spectrum between Discharge and Cloud Cover')\n",
    "    ax['r'].set_ylim(-180, 180)  # Phase difference ranges from -180 to 180 degrees\n",
    "    \n",
    "    # plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return f\n",
    "\n",
    "# Example usage for a specific site\n",
    "site_2_plot = sites.sample()\n",
    "f = spectral_analysis_site(df, site_2_plot.index[0])\n",
    "site_2_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542212af-406a-4810-b9ed-33f7f4f3a3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cfbe10-f84b-417d-8817-a527225e629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch, coherence, csd\n",
    "\n",
    "idx = []  \n",
    "n_groups = len(df.index.get_level_values('id').unique())\n",
    "for i,g in tqdm(df.groupby('id'),total=n_groups):\n",
    "        g = g.droplevel('id')\n",
    "\n",
    "        #number of valid days per year. Excludes missing data or ice flagged days.\n",
    "        # nperseg = 365.25*len(g)/(g.index.max()-g.index.min()).days\n",
    "        # if nperseg < 180:\n",
    "        #     continue\n",
    "            \n",
    "        nperseg = 90\n",
    "\n",
    "        if (len(g) < nperseg):\n",
    "            continue\n",
    "        \n",
    "        # Extract discharge (Q) and cloud cover (cloudMask) columns\n",
    "        discharge = g['Q'].values\n",
    "        cloud_cover = g['cloudMask'].values\n",
    "\n",
    "        # Spectral analysis\n",
    "        fs = 1  # Sampling frequency (1 sample per day)\n",
    "        f_discharge, Pxx_discharge = welch(discharge, fs=fs, nperseg=nperseg, scaling='density')\n",
    "        f_cloud, Pxx_cloud = welch(cloud_cover, fs=fs, nperseg=nperseg, scaling='density')\n",
    "        f_coherence, Cxy = coherence(discharge, cloud_cover, fs=fs, nperseg=nperseg)\n",
    "\n",
    "        # Calculate cross spectral density which contains phase information\n",
    "        f, Pxy = csd(discharge, cloud_cover, fs=fs, nperseg=nperseg)\n",
    "        \n",
    "        \n",
    "        count = len(idx)\n",
    "        if count==0:\n",
    "            phase = np.full((Cxy.size,n_groups),np.nan)\n",
    "            power = np.full((Cxy.size,n_groups),np.nan)\n",
    "            coh = np.full((Cxy.size,n_groups),np.nan)\n",
    "        phase[:,count] = np.angle(Pxy)\n",
    "        power[:,count] = Pxx_discharge\n",
    "        coh[:,count] = Cxy\n",
    "        \n",
    "        idx.append(i)\n",
    "\n",
    "#Select columns that are not all NaN\n",
    "phase = phase[:, ~np.all(np.isnan(phase), axis=0)]\n",
    "coh = coh[:, ~np.all(np.isnan(coh), axis=0)]\n",
    "\n",
    "\n",
    "# tmp = pd.DataFrame({'CSD_phase':phase,'spec_coh':coh},index=idx)\n",
    "# sites_plot = sites.loc[:, ~sites.columns.isin(['CSD_phase','spec_coh'])]\n",
    "# sites_plot = sites_plot.join(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9634cc8e-0c58-43e5-86b9-c3f1afed1797",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_mean = np.nanmean(phase, axis=1)\n",
    "power_mean = np.nanmean(power, axis=1)\n",
    "coh_mean = np.nanmean(coh, axis=1)\n",
    "\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots()\n",
    "# Create a pseudocolor plot\n",
    "cax = ax.pcolor(coh[1:,:], edgecolors='none', linewidths=1, vmin=0, vmax=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4cf88d-fecd-41d3-916e-3c040b3a87de",
   "metadata": {},
   "outputs": [],
   "source": [
    "coh.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d62ead-46cb-4ba9-ac5e-302b4e4d1dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "norm = TwoSlopeNorm(vmin=-0.3, vcenter=0, vmax=0.3)\n",
    "cmap = 'RdBu'\n",
    "# thresh = 0\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "ax.scatter(sites_plot['spec_coh'],\n",
    "            np.abs(sites_plot['CSD_phase']),\n",
    "            c=sites_plot['normDiff_q90'],s=0.5,norm=norm, cmap='RdBu',edgecolor=None)\n",
    "ax.set_ylabel(\"Phase offset\")\n",
    "ax.set_xlabel(\"Mean amplitude\")\n",
    "ax.set_xlim([0.1,1])\n",
    "# ax.set_facecolor([0.75]*3)\n",
    "\n",
    "plt.subplots_adjust(left=0.1, right=0.8)\n",
    "cax = fig.add_axes([0.85, 0.15, 0.03, 0.7])\n",
    "cbar = fig.colorbar(mp.cm.ScalarMappable(norm=norm, cmap=cmap), cax=cax)\n",
    "# cbar.set_ticks([-0.4, -0.2, 0, 0.2, 0.4])\n",
    "cbar.set_label(\"Norm. diff. of median discharge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259c7f4b-4722-4741-bd61-95e6d8560dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72566f91-f91e-4bbf-9ba1-2ae120902ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 2D histogram bins\n",
    "x = sites_plot['spec_coh']\n",
    "y = np.abs(sites_plot['CSD_phase'])\n",
    "z = sites_plot['normDiff_q90']\n",
    "\n",
    "norm = TwoSlopeNorm(vmin=-0.5, vcenter=0, vmax=0.5)\n",
    "cmap = 'RdBu'\n",
    "\n",
    "# Create 2D histogram for binning the x and y data\n",
    "x_edges = np.linspace(0, 1, 9)\n",
    "y_edges = np.linspace(0, np.pi, 9)\n",
    "arr_size = (len(x_edges)-1, len(y_edges)-1)\n",
    "\n",
    "x_center = (x_edges[:-1] + x_edges[1:]) / 2\n",
    "y_center = (y_edges[:-1] + y_edges[1:]) / 2\n",
    "xx, yy = np.meshgrid(x_center, y_center, indexing='ij')\n",
    "\n",
    "# Create an array to store the average z values for each bin\n",
    "z_avg = np.full(arr_size, 0, dtype=np.float64)\n",
    "z_count = np.full(arr_size, 0, dtype=np.float64)\n",
    "\n",
    "# Populate the array with the average z values\n",
    "for i in range(arr_size[0]):\n",
    "    for j in range(arr_size[1]):\n",
    "        mask = (x >= x_edges[i]) & (x < x_edges[i + 1]) & (y >= y_edges[j]) & (y < y_edges[j + 1])\n",
    "        if np.sum(mask)>0:\n",
    "            z_avg[i, j] = np.mean(z[mask])\n",
    "            z_count[i, j] = np.sum(mask)\n",
    "\n",
    "# Plot the 2D histogram with the average z values\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(figsize=(5.5, 3.5))\n",
    "\n",
    "ax.set_xlabel(\"Coherence\")\n",
    "ax.set_ylabel(\"Cross Spectral Density Phase\")\n",
    "ax.set_ylim([0,np.pi])\n",
    "ax.set_yticks(np.pi*np.array([0,1/4,1/2,3/4,1]))\n",
    "ax.set_yticklabels(['0', r'$\\frac{1}{4}\\pi$', r'$\\frac{1}{2}\\pi$', r'$\\frac{3}{4}\\pi$', r'${\\pi}$'])\n",
    "# ax.set_xticks([0.1,0.2,0.3,0.4,0.5])\n",
    "# ax.set_xlim([0,20])\n",
    "\n",
    "RESCALE = 4\n",
    "scatter = ax.scatter(xx,yy,c=z_avg,s=z_count/RESCALE,cmap=cmap,norm=norm)\n",
    "\n",
    "handles, labels = scatter.legend_elements(\"sizes\",num=6,alpha=0.5)\n",
    "#Extract the numeric labels and rescale them\n",
    "rescaled_labels = [f\"{int(re.search(r'\\d+', label).group()) * RESCALE:.0f}\" for label in labels]\n",
    "legend2 = ax.legend(handles, rescaled_labels, \n",
    "                    loc=\"upper left\", \n",
    "                    title=\"River\\nreaches\", \n",
    "                    labelspacing=1.5,\n",
    "                    frameon=False,\n",
    "                    # borderpad=1,\n",
    "                    bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Adjust subplot and add colorbar\n",
    "plt.subplots_adjust(left=0.12, right=0.6, bottom=0.15)\n",
    "cax = fig.add_axes([0.8, 0.15, 0.03, 0.7])\n",
    "cbar = fig.colorbar(scatter,cax=cax)\n",
    "cbar.set_label('ND of median')\n",
    "cbar.set_ticks([-0.4, -0.2, 0, 0.2, 0.4])\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40259b8b-902a-4ff6-ac25-62044866a3da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2da751-4c82-4e9d-beab-f1f1207240b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.hist(sites_plot['spec_power'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d814ff2-abc4-4f70-81a8-c45010c3af85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db381a-bf3e-4307-9945-5de3ca4e3b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "# fig, axes = plt.subplots(1,2,figsize=(8,3))\n",
    "# axes = axes.flatten() # Flatten the axes array for easy iteration\n",
    "\n",
    "plt.hist(sites.normDiff_q50, range=(-1, 1), bins=41, density=True)\n",
    "plt.hist(sites[sites.stationid.notna()].normDiff_q50, range=(-1, 1), bins=41, density=True,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237288a-bfa1-4e79-b083-fa3c20330ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e81981-498a-471b-ba02-13fda487f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "norm = TwoSlopeNorm(vmin=-0.5, vcenter=0, vmax=0.5)\n",
    "cmap = 'RdBu'\n",
    "\n",
    "X = sites.drop(columns=['strmDrop_t','slope_taud','maxup','up1','up2','up3','up4','l1','l2','NextDownID','lengthdir','CSD_phase','CSD_magnitude','coherence','pIce'])\n",
    "numeric_columns = X.select_dtypes(include=[np.number]).columns\n",
    "X = X[numeric_columns]\n",
    "X = X.dropna()\n",
    "\n",
    "y = X['normDiff_q50']\n",
    "X = X[[col for col in X.columns if not col.startswith('normDiff')]]\n",
    "\n",
    "X_standardized = (X - X.mean()) / X.std()\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=8)\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Identify predictive variables using Linear Regression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_standardized, y)\n",
    "coefficients = pd.Series(reg.coef_, index=pca.feature_names_in_)\n",
    "\n",
    "# Create plots\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Scatter plot of PCA components\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, s=0.1, cmap=cmap, norm=norm)\n",
    "plt.colorbar()\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('PCA Components')\n",
    "\n",
    "# Coefficient plot\n",
    "plt.subplot(1, 2, 2)\n",
    "coefficients.plot(kind='barh')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Coefficients for Predicting normDiff_q50')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec03e8-d953-48cf-adbe-d154c069ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13866744-d9f4-4cef-a7b4-8cc42128adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pca.components_\n",
    "strength = pca.explained_variance_\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.1\n",
    "\n",
    "# Create a bar chart with grouped bars\n",
    "plt.figure(figsize=(8, 4))\n",
    "ax = plt.gca()\n",
    "n_features = loadings.shape[1]\n",
    "index = np.arange(n_features)\n",
    "for idx in range(len(loadings)):\n",
    "    ax.bar(index+(bar_width*idx), loadings[idx]*strength[idx], bar_width)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Loadings')\n",
    "plt.title('PCA Loadings')\n",
    "plt.xticks(index + bar_width, pca.feature_names_in_, rotation=90)\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea2e958-a94d-495d-85d8-a53ea9d9c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b8a655-0bf6-4dd2-97ab-1b55318c4efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the PCA loadings\n",
    "loadings = pca.components_\n",
    "n_comp = len(loadings)\n",
    "\n",
    "# Calculate the overall strength of each axis\n",
    "strength_axis = pca.explained_variance_ratio_\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.bar(np.linspace(1,n_comp,n_comp),strength_axis)\n",
    "plt.xlabel('PCA Axis')\n",
    "plt.ylabel('Overall Strength')\n",
    "plt.title('Overall Strength of Each PCA Axis')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tss-ml]",
   "language": "python",
   "name": "conda-env-.conda-tss-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
